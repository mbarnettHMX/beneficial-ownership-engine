{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%configure -f\n",
        "{\n",
        "    \"conf\":\n",
        "    {\n",
        "        \"spark.sql.shuffle.partitions\": 16,\n",
        "        \"spark.sql.broadcastTimeout\": 14400,\n",
        "        \"spark.port.maxRetries\": 100\n",
        "\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from typing import Dict\n",
        "from transparency_engine.pipeline import TransparencyPipeline\n",
        "from transparency_engine.synthetic_data import public_procurement\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "#Storage Config\n",
        "storageLinkedService = 'LS_Data'\n",
        "storageAccount_ls = mssparkutils.credentials.getPropertiesAll(storageLinkedService)\n",
        "storage_account = json.loads(storageAccount_ls)['Endpoint'].split('.')[0].replace('https://','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# def generate_procurement_data(\n",
        "#     config: DataGeneratorConfig = procurement_configs,\n",
        "#     n_entities: int = 1000,\n",
        "#     n_communities: int = 20,\n",
        "#     n_periods: int = 20,\n",
        "# )\n",
        "\n",
        "#     Parameters:\n",
        "#         config\n",
        "#         n_entities: int, default = 1000\n",
        "#             Number of entities to generate\n",
        "#         n_communities: int, default = 20\n",
        "#             Number of communities in the entity graph.\n",
        "#         n_periods: int, default = 20\n",
        "#             Number of observed periods\n",
        "#     Returns: Dictionary containing 7 dataframes to be used as inputs for the transparency engine model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "sample_data_dict = public_procurement.generate_procurement_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "for table in sample_data_dict:\n",
        "    output = sample_data_dict[table].coalesce(1).write.option(\"header\",\"true\").csv('abfss://landing@'+storage_account+'.dfs.core.windows.net/BeneficialOwnership/GeneratedData/temp/'+table)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
